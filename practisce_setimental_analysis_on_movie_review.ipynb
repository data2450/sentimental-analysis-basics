{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "practisce setimental analysis on movie review.ipynb",
      "provenance": [],
      "mount_file_id": "15utvGhQisyssOuMI7PCs9wzVkE5gA_t3",
      "authorship_tag": "ABX9TyPLm1hLZBO0vKI91OvH3y5e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/data2450/sentimental-analysis-basics/blob/main/practisce_setimental_analysis_on_movie_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9B94mNxe2MP"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count= CountVectorizer()\n",
        "\n",
        "\n",
        "docs=np.array([ 'The sun is shining',\n",
        "                'The weather is sweet',\n",
        "                'The sun is shining, the weather is sweet, and one and one is two'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_XYHbatopgf"
      },
      "source": [
        "**Bag of words**:way to represent text for use in ML ,which ignores the structure and only counts how often each word occure \n",
        "\n",
        "**count vectorizer** allows us to use the **bag of words** approach by converting collection of text document into matrix of token count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czWALJi_oaFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebb14f5-d44d-4067-fee2-64ccccfce323"
      },
      "source": [
        "#now the text document is inside the array\n",
        "docs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['The sun is shining', 'The weather is sweet',\n",
              "       'The sun is shining, the weather is sweet, and one and one is two'],\n",
              "      dtype='<U64')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6cpmnKBGRm7"
      },
      "source": [
        "bag=count.fit_transform(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nf0pA5Xzojg7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4922613-0ef2-46ea-b290-5b6ef1a10c7c"
      },
      "source": [
        "bag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3x9 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 17 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcRjCw88qWmN"
      },
      "source": [
        "# Sparse Matrix\n",
        "A sparse matrix is a matrix that is comprised of mostly zero values.\n",
        "\n",
        "Sparse matrices are distinct from matrices with mostly non-zero values, which are referred to as dense matrices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B0c5tbsGVFS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aedf23dd-42c2-4b01-8e26-0aaa5f553dc6"
      },
      "source": [
        "print(count.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqooGPt_H4r5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "769cc228-e287-43ff-8ed4-965df835b0b6"
      },
      "source": [
        "print(bag.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 1 1 0 1 0 0]\n",
            " [0 1 0 0 0 1 1 0 1]\n",
            " [2 3 2 1 1 1 2 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maTLNML54g4N"
      },
      "source": [
        "the first index is 'and' and if check the first column we can see the first element is zero sine the word 'and' is not present in the first sentance and 2nd sentance , in 3rd sentance \"and\" is ocurring 2 times so the 3rd value in the column will be 2 \n",
        "\n",
        "2nd column of the matrix represnt the word \"is\" present in 1st and 2nd sentance once and 3 times in 3rd sentance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07KOFugPzqkf"
      },
      "source": [
        "**Task 3: Word relevancy using term frequency-inverse document frequency**\n",
        "\n",
        "\n",
        "\n",
        "**TF**: Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization\n",
        "\n",
        "**TF(t)** = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
        "\n",
        "**IDF**: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
        "\n",
        "**IDF(t)** = log_e(Total number of documents / Number of documents with term t in it).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kNjQXy_jztX"
      },
      "source": [
        "**TF-IDF** =  **TF * IDF** \n",
        "\n",
        "allows as to weight terms based on how important they are to the document .High weight is given to terms that appear often in a particular document .But don't appear ofen in the corpus . features with low tfidf are either commonly used across all documents or rarely used and only occur in long document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1wMFoKwyUW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e81e788b-cbe6-4b27-dc7a-223870b53b3a"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "np.set_printoptions(precision=2)\n",
        "tfidf=TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
        "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
            " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
            " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPbHzuF0jaKz"
      },
      "source": [
        "so we can see that the word \"is\"  has score 0.43 because the word is occuring in all the sentance so that word is given less importamce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fALxDyXupwVJ"
      },
      "source": [
        "# Tokanisation of documents\n",
        "In this section, we will represent our data as a collection of words or tokens; we will also be performing word-level preprocessing tasks such as stemming. To achieve this, we will utilize the natural language toolkit, or nltk.\n",
        "\n",
        "Stemming is a technic that reduces the inflectional forms, and sometimes derivationally related forms, of a common word to a base form. For example, the words ‘organizer’ and ‘organizing’ stems from the base word ‘organize’. So, stemming is conventionally referred to as a crude heuristic process that ‘chops’ off the ends of words, in the hope of achieving the goal correctly most of the time — this often includes the removal of derivational affixes.gives as the root of the word from derivation of that word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIGanGaN089g"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "porter=PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJnCzMwwp9ZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "120cfa76-3995-497c-9a14-bfbd5ca4c7e0"
      },
      "source": [
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "tokenizer('runners like running thus they run')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runners', 'like', 'running', 'thus', 'they', 'run']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhYxXevoqBSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e0df31b-0648-4941-82c4-cdf20ad2b466"
      },
      "source": [
        "def tokenizer_stemmer(text):\n",
        "    return[porter.stem(word) for word in text.split()]\n",
        "tokenizer_stemmer(\"runners like running thus they run\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['runner', 'like', 'run', 'thu', 'they', 'run']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwkZevSEqJnM"
      },
      "source": [
        "stopword\n",
        "\n",
        "commomn words that always occur in a sentance like \"is\" \"a\" \"and\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "640nMVS3qm9O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0688480d-f747-4896-e1a0-606cd65dbe16"
      },
      "source": [
        "! pip install nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erT9Ob5CqMgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c35a607-3e86-48cc-81d3-8fafc1ed576e"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_w=stopwords.words(\"english\")\n",
        "[w for w in tokenizer_stemmer('ship and lion in is and sleep sleeps with sleeping')[-11:]if w not in stop_w]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ship', 'lion', 'sleep', 'sleep', 'sleep']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}